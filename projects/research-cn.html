<!DOCTYPE HTML>

<html>

<body bgcolor="#EEEEEE">

<div style="background:#FFFFFF; width:880px; align:center; text-align:justify; margin:auto;padding:30px">
  
	
<a name="research" align="center"><h1>研究综述</h1></a>
<br><br>
三维可视信息是人类或机器人互相理解和交流以及和真实物理世界交互的的主要媒介。在当前大量的新兴应用领域中，如各类无人系统、混合现实和数字内容创作等，<b><i>可视计算和学习</i></b>技术起到越来越关键的支撑和推进作用。 

<br><br>
本研究涵盖了可视计算和学习的多个热门领域，如<b><i>几何深度学习、三维视觉以及传统几何建模和分析</i></b>。总研究目标为理解三维对象的构造过程来进行三维建模和分析，以及探索二维视觉观测与三维物理世界之间的关联来辅助人与机器人或机器人之间的交互。近一年多，我们先后在三维生成任务中引入一些新的概念和思路，如序列化部件表征和多模态形体补全，来生成结构保持及几何多样的三维形体，同时我们引入了一些十分有挑战的应用场景，拓展了场景建模和图像定位问题的前沿研究，如提出了基于小样本观测的相机重定位问题以及面向持续变化场景的三维建模问题。后面三个章节将分别介绍了三个主要研究领域的研究目标和已取得的研究成果。
<br><br>
本项目重点研究了对象和场景的三维几何和结构等内在属性以及物体和相机位姿等外在属性，未来我们将集成纹理、材质和光照等更多内在属性，进行全要素的三维重建以及面向虚拟化现实的对象和场景生成。其他未来研究还包括运用深度学习工具对传统纯几何问题进行改造，寻找高效、鲁棒的解决方案。例如许多几何优化问题中，传统的启发式策略以及优化求解可以通过表达能力更强的深度网络来自动学习和拟合。此外，许多传统问题本质上还存在着不确定性或歧义性，输出目标蕴含多种可能性，这些特性普遍存在于真实世界当中，相对于传统的确定性方法，基于数据分布建模的深度模型在这类问题上更具优势。
<br><br>
总而言之，可视计算和学习正在重塑人们交流、娱乐以及工业制造等领域。类似于许多成功的VR、AR和数字内容创作等应用产品，我们的最终目标是打造软硬件结合的产品，将我们的可视计算和学习算法融入其中，逐渐提高产品的智能水平和易用性，更好的服务人们的日常生活和生产任务。

<br><br>

<!--<a name="rep research" align="center"><h2>代表性研究课题</h2></a>-->


<a name="I. Geometric Deep Learning" ><h3>一、几何深度学习 (2019-2020)</h3></a>

<font size="4"></font>
本课题在几何深度学习领域的主要研究是理解三维形体的构建过程来服务三维形体生成任务，获取高质量和多样化的三维几何。核心思想是建立三维几何和其他模态的内在联系，包括自然语言和文本、图像和视频以及草图绘画等模态。<i>通过更加低维度、更抽象的模态进行三维内容的原型设计，借助多模态数据中蕴含的模态关联信息，以及建立<b>设计空间</b>到实物空间之间的映射，来生成符合原型设计、结构准确、细节丰富的几何形体。</i>我们首先尝试通过类似于文本中句子的形式来表征形体，利用RNN网络来合成三维内容。同时，我们第一次运用条件生成网络对过程式建模和形体补全任务中的形体尚未完成区域或缺失区域进行数据分布建模，为每个未完成的形体提供多种补集方案，组成完整、真实且多样化的形体。更多关于形体抽象设计与设计空间到三维实体空间的映射学习正在进行中。这部分研究始于2019年，现有成果完成于北京大学前沿计算研究中心，由我的合作导师陈宝权教授领导。 此研究受国家自然科学基金青年项目资助（项目号：61902007；项目名称：<b>高分辨率和结构保持的深度几何模型表达与生成</b>），庄一新为项目负责人。

<br><br>

	<table ><tbody><tr>
	<td valign="center" align="center"  width="400" id="PQ-NET"><a href="http://www.yixin.io/projects/research.html"><img src="../images/thumbs/pq-net.jpg" width="370" height="200" border="0"></a>
	</td>
	<td>
	<b>PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes</b>
   	<br>
	CVPR 2020
	<br><br>
	<i>核心思路</i>: 我们提出一种深度神经网络，通过逐个部件组装的方式来生成三维形体。这种部件序列化的形体表征方式类似于自然语言中的句子形式，非常恰当地描述了三维形体的构造过程。该想法主要来源于Biederman的经典论述，既recognition-by-components (RBC)，也就是说语言构造与形体感知之间有着本质上的相似性。我们采用类似于NPL翻译中常用的RNN网络，理解和学习PartNet数据集内三维形体的部件组装规律，并用于生成部件、部件关系及整体部件组装过程。
	<br>
	<font color="336699">
	<b><i>未来工作</i></b>：该方法提供了通用的形体生成网络，未来可以支持从抽象设计，如文本和草图等，生成具体的三维内容。此问题的核心在于构造跨模态元素的联合分布表征，实现在不同模态间“翻译”，如把句子或草图翻译成三维形体。
	</font>
	</td>
	</tr></tbody></table> 
	
<br><br>

	<table><tbody><tr>
	<td valign="center" align="center" width="400" id="multi-completion"><a href="http://www.yixin.io/projects/research.html"><img src="../images/thumbs/multi-completion.jpg" width="400" height="234" border="0"></a>
	</td>
	<td>
	<b>Multimodal Shape Completion via Conditional Generative Adversarial Networks</b>
  	<br>
	ECCV 2020 (Spotlight)
	<br><br>
	<i>核心思路</i>: 交互式建模、过程式建模以及形体补全等问题需要预测半成品几何体中未完成的或者缺失的区域，并生成适当的补集来构造完整形体。已有基于深度生成网络的方法只提供一种补全结果，但是实际建模中，未知区域是不确定的，也不是唯一的。我们第一次提出对这种不确定性或者歧义性进行建模，为形体补全提供多种选择。我们设计条件对抗生成网络，从数据集中提取每个缺失形体的多样性补全结果。该方法不需要数据集提供每个补全对应的结果，而是通过非匹配的弱监督方式训练网络参数。
	<br>
	<font color="336699">
	<b><i>未来工作</i></b>：该方法对不确定性建模，提供基于多样性生成的补全方案。但是目前是一次性生成完整形体，未来我们考虑逐部件的生成，每一步补全提供多样化的部件选择，从而序列化、可视化整个多样化的三维生成过程。多样化部件组装是理解形体内部构造原理的崭新思路。另一个未来研究是对来自不同类别对象进行融合、“杂交”，生成从未见过的新事物，突破目前形体生成结果仍实际上受限于数据集内插值的限制。
	</font>
	</td>
	</tr></tbody></table> 


<br><br>

<a name="II. 3D Vision" ><h3>二、三维视觉 (2019-2020)</h3></a>

本课题关于三维视觉的研究主要集中在基于图像的三维重建和定位问题。核心内容在于通过非结构化或者时序化的二维颜色、深度图像为媒介，建立观测者、图像和真实世界之间的关联，来计算观测者的位姿和物理世界的三维几何信息。其主要难点在于观测数据数量小、质量低和环境持续变化等挑战，瓶颈在于终端的计算性能。因此，我们于近期提出若干个面向当前迫切需求和前沿应用的研究问题，如面向只有小样本场景图像的相机重定位问题、面向持续变化场景的渐进式三维建模问题以及一些和机器人相关的视觉感知和定位问题等。目前已在小样本定位问题上取得初步研究成果，持续变化场景的建模问题正在重点推进。这些研究将使许多使用消费级相机的场景受益，如使用移动手机和行车记录仪等场景，研究成果将在小样本和持续变化等条件下，提供建模和定位服务。这部分研究始于2019年，现有成果完成于北京大学前沿计算研究中心，由我的合作导师陈宝权教授领导。此研究受国家重点研发计划项目资助（项目号：2018YFB1403901、2019YFF0302902），庄一新为项目骨干。

<br><br>

	<table><tbody><tr>
	<td valign="center" align="center" width="400" id="fewshot-reloc"><a href="http://www.yixin.io/projects/research.html"><img src="../images/thumbs/fewshot-reloc.png" width="370" height="370" border="0"></a>
	</td>
	<td>
	<b>Decoupling Features and Coordinates for Few-shot RGB Relocalization</b>
  	<br>
	arXiv preprint 2020
	<br><br>
	<i>核心思路</i>: 相机重定位需要提供已知场景大量观测数据，并逐个场景进行训练，因而跨场景模型自适应具有非常大需求。最新的方法在小样本场景自适应中效果较差，主要原因在于将观测图像和世界坐标深度耦合，因而图像特征学习受具体场景影响较大，而为了得到较好预测效果，需要大量数据重新进行特征学习。我们解决此问题的思路来源于模仿人类重定位的过程，既视觉特征感知、世界坐标回归及位姿估计是分离的。因为物体在世界坐标系下的坐标与观测视角无关，我们在学习图像特征阶段屏蔽场景的坐标。通过如此解耦，学习到场景的通用特征，且新场景只需要提供小样本视角图像既可得到较好的逐像素坐标预测，最后位姿估计结果也比传统方法有两个数量级的提升。
	<br>
	<font color="336699">
	<b><i>未来工作</i></b>：跨场景自适应定位是非常基础的三维建模模块。基于以上成果，当用户进入新环境，只需要快速获取几张图像，就可以在新场景中进行定位。后续研究之一是继续提升小样本自适应的定位精度，另一方面是研究持续变化场景下的定位技术，通过场景扰动，改变场景中部分物体的位姿，以及增加动态人群，来学习判断场景的“变”与“不变”区域，提供稳定的定位结果。而准确的定位反过来可以对场景变化区域进行更新。
	</font>
	</td>
	</tr></tbody></table> 

<br><br>


<a name="III. Shape Processing and Modeling" ><h3>三、几何建模和处理(2012-2016)</h3></a>

本课题借助草图来研究几何建模和处理。草图是对象的最紧致几何表征之一，其通过简单的线画勾勒出对象的大体形态，并通过人类的视觉认知，推测出蕴含于线条背后的三维对象的复杂拓扑和几何细节。此研究的目的包括两个方面：一是从用户设计的自由草图出发，计算最接近用户设计意图的几何曲面；反之，另一方面则寻找对象表面几何的最有代表性的曲线网络。与基于深度学习的方法不同，这部分研究采用传统的研究框架，研究方案结合了低层级的微分几何计算和高层级的领域知识设计和策略优化。这部分研究完成于2012-2016年，为庄一新博士期间研究成果。 

<br><br>

	<table><tbody><tr>
	<td valign="center" align="center" width="400" id="CycleDisc"><a href="http://www.yixin.io/projects/research.html"><img src="../images/fulls/CycleDisc_teaser2.png" width="380" height="310" border="0"></a>
	</td>
	<td>
	<b>A General and Efficient Method for Finding Cycles in 3D Curve  Networks</b>
  	<br>
	ACM Transaction on Graphics (Siggraph Asia 2013)
	<br><br>
	<i>核心思路</i>: 基于三维曲线网络的对象表面网格生成是计算机图形学的经典问题，近些年来在这方面比较流行的研究是基于草图的曲面建模系统。从草图中计算三维曲面可分为两个步骤，第一步是确定拓扑，用线圈把曲面分成片状区域，然后在每个线圈内部生成三角网格。该工作的核心贡献在于提出一种新的寻找线圈的方法。与此前方法不同的是，我们方法得到的拓扑没有任何约束限制，可以是任意亏格的流形或非流形曲面。核心思路是将基于图的线圈搜索问题重新表达成图的顶点和边的局部映射问题，其中每个映射描述了相连的两条边的连通代价，并通过全局优化选择满足最小连接代价的线圈集。该方法非常高效，可以嵌入交互式设计系统中，用户可以通过调整线圈连接方式，或者改变边的流形属性来优化拓扑。
	</td>
	</tr></tbody></table>   

<br><br>

	<table><tbody><tr>
	<td valign="center" align="center" width="400" id="3DLiveWire"><a href="http://www.yixin.io/projects/research.html"><img src="../images/fulls/livewire_teaser2.png" width="380" height="320" border="0"></a>
	</td>
	<td>
	<b>Anisotropic Geodesics for Live-wire Mesh Segmentation</b>
  	<br>
	Computer Graphics Forum (Pacific Graphics 2014)
	<br><br>
	<i>核心思路</i>: 网格分割等价于在曲面上寻找封闭的曲线网络。受二维图像分割中Live-wire工具启发，我们提出一种交互式网格分割方法，通过在网格上交互式选择一系列点，并自动寻找连接顺序两个节点间的连线，来构成封闭的曲线网络。该方法的核心贡献在于连线的定义和计算。我们定义一种新的各向异性度量下的测地线作为连线，兼顾了平滑性和特征敏感性。同时，我们提出一种新的在三角网格上计算各向异性测地线的算法。对于有显著特征区域的模型，通过我们方法能够快速获得边界精确的分割结果。通过用户学习，我们的方法和现有网格分割工具相比，在易用性和效率上都有极大提升。
	</td>
	</tr></tbody></table> 

 <br><br>

	<table><tbody><tr>
	<td valign="center" align="center" width="400" id="FeatureSeg"><a href="http://www.yixin.io/projects/research.html"><img src="../images/fulls/FeatureSeg2.png" width="380" height="309" border="0"></a>
	</td>
	<td>
	<b>Feature-Aligned Segmentation using Correlation Clustering</b>
  	<br>
	Computational Visual Media 2017 (best paper honorable mentioned)
	<br><br>
	<i>核心思路</i>: 我们提出一种基于面片表示的网格分割方法，能够自动生成和曲面显著特征对齐的分割线。核心思路在于将分割问题转化为关联聚类问题。关联聚类问题首先在数据挖掘领域被提出，与传统的基于聚类的网格分割方法相比，该方法最大优势是无参数的，不需要指定分割的类数。关联聚类的输入为带正负边的权重图，负权重边的构造可以通过特征线段自动检测或者用户交互式草图输入来控制，从而影响聚类细节。关联聚类求解代价非常高，我们提出一种加速的算法，在小于10万个顶点的网格上，能够达到实时且接近最优关联聚类的结果。与已有方法相比，我们得到的分割结果边缘与曲面特征更加对齐，非常适合纹理贴图、网格抽象等应用。
	</td>
	</tr></tbody></table> 

<br><br>


	<table><tbody><tr>
	<td valign="center" align="center" width="400" id="GeoTopo"><a href="http://www.yixin.io/projects/research.html"><img src="../images/fulls/geotopo2.png" width="380" height="277" border="0"></a>
	</td>
	<td>
	<b>Deformation-Driven Topology-Varying 3D Shape Correspondence</b>
  	<br>
	ACM Transaction on Graphics (Siggraph Asia 2015)
	<br><br>
	<i>核心思路</i>: 
	我们提出一种基于最优变形代价的不同拓扑形体匹配算法。也就是说，按照最优匹配结果，从一个形体变形到另外一个可能具有不同拓扑的形体，其变形代价应该最低。匹配算法的核心部分是变形能量函数，该能量惩罚变形后的几何扭曲，鼓励变形过程结构保持，能适用于不同拓扑对象间的匹配。其实现采用结构杆模型来连接形体部件，该设计类似于虚拟弹簧模型，但是同时可计算和存储几何、结构和拓扑变化的能量值。基于此形体表达，最后采用集束算法来寻找部件变形累积能量最低的最优形体匹配。
	</td>
	</tr></tbody></table> 


<!--
<table><tbody><tr>
	<td valign="center" align="center" width="880" id="GeoTopo"><a href="http://www.yixin.io/projects/research.html"> www.yixin.io/projects/research.html </a> 
	</td>
	</tr></tbody></table> 
-->

</div>
</body>
</html>

